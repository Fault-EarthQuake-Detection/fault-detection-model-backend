{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1jutoH4nmo5inHvI7ub9cvcdHKaJi_Os-","authorship_tag":"ABX9TyMkRsKJkN9ukrFUIRkTsDpO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **koneksi ke drive**"],"metadata":{"id":"Dd_Z83RTncge"}},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"moQVWZzfkrcL","executionInfo":{"status":"ok","timestamp":1759340082757,"user_tz":-420,"elapsed":1997,"user":{"displayName":"Fikal Alif","userId":"10624786544846002244"}},"outputId":"afe1c39c-a2ce-4400-93d5-7542818ad647"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# **install library**"],"metadata":{"id":"_idLGSc6nqYM"}},{"cell_type":"code","source":["!pip install fastapi pyngrok \"uvicorn[standard]\" python-multipart \"albumentations==2.0.8\" \"segmentation_models_pytorch==0.5.0\" \"huggingface-hub==0.35.1\" \"timm==1.0.20\" --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpI4WtMLn04V","executionInfo":{"status":"ok","timestamp":1759336746403,"user_tz":-420,"elapsed":10407,"user":{"displayName":"Fikal Alif","userId":"10624786544846002244"}},"outputId":"dd8f75b1-a124-4ef1-b7fd-34840d607e54"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m126.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["# **push github**"],"metadata":{"id":"ecNHzs49n7eW"}},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# 🔹 Mount Google Drive (kalau file ada di Drive)\n","drive.mount('/content/drive')\n","\n","# 🔹 Setup variabel akun GitHub\n","USERNAME = \"Fault-EarthQuake-Detection\"   # ganti dengan username kamu\n","REPO = \"fault-detection-model-backend\"\n","TOKEN = 'GITHUB_PAT'    # ganti dengan token GitHub kamu\n","\n","# 🔹 Clone repo pakai token (biar sekalian bisa push)\n","!git clone https://{USERNAME}:{TOKEN}@github.com/{USERNAME}/{REPO}.git\n","%cd {REPO}\n","\n","# 🔹 Copy file project dari Drive ke repo\n","!cp /content/drive/MyDrive/Colab_Notebooks/model_fault_detection.ipynb .\n","# !cp /content/app.py .\n","# !cp -r /content/models .\n","\n","# 🔹 Config user Git\n","!git config --global user.email \"fikalal\"\n","!git config --global user.name \"{USERNAME}\"\n","\n","# 🔹 Add, commit, push\n","!git add .\n","!git commit -m \"add training script + fastapi app\"\n","!git push origin main\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZddF1xBoClN","executionInfo":{"status":"ok","timestamp":1759340277590,"user_tz":-420,"elapsed":3887,"user":{"displayName":"Fikal Alif","userId":"10624786544846002244"}},"outputId":"520c69f3-04d0-4764-9f00-9d5d0cdd153e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Cloning into 'fault-detection-model-backend'...\n","warning: You appear to have cloned an empty repository.\n","/content/fault-detection-model-backend/fault-detection-model-backend/fault-detection-model-backend/fault-detection-model-backend/fault-detection-model-backend/fault-detection-model-backend\n","[main (root-commit) 711054e] add training script + fastapi app\n"," 1 file changed, 1 insertion(+)\n"," create mode 100644 model_fault_detection.ipynb\n","fatal: could not read Username for 'https://github.com': No such device or address\n"]}]},{"cell_type":"markdown","source":["# **parsing and check dataset**"],"metadata":{"id":"gW2AY7ZooHtZ"}},{"cell_type":"markdown","source":["check dataset"],"metadata":{"id":"gjMPWzz7r6Er"}},{"cell_type":"code","source":["import pandas as pd\n","from pathlib import Path\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# dataset path\n","DATASET_DIR = Path(\"/home/fikal/fikal/fault_detection/fault-detection-backend/dataset_fault\")\n","PATCH_DIR = DATASET_DIR / \"patched_images\"\n","\n","# load csv\n","train_df = pd.read_csv(\"train.csv\")\n","\n","# fungsi convert path\n","def fix_path(path_str):\n","    # ambil nama file aja\n","    filename = path_str.split(\"\\\\\")[-1]\n","    return PATCH_DIR / filename\n","\n","# convert kolom\n","train_df[\"image\"] = train_df[\"Original Image Patch\"].apply(fix_path)\n","train_df[\"mask\"]  = train_df[\"Binary Mask Image Patch\"].apply(fix_path)\n","\n","print(train_df.head())\n","\n","# cek contoh gambar\n","img_path = train_df.iloc[0][\"image\"]\n","mask_path = train_df.iloc[0][\"mask\"]\n","\n","print(\"Image:\", img_path)\n","print(\"Mask:\", mask_path)\n","\n","# plot\n","img = Image.open(img_path)\n","mask = Image.open(mask_path)\n","\n","plt.subplot(1,2,1)\n","plt.imshow(img)\n","plt.title(\"Original Patch\")\n","\n","plt.subplot(1,2,2)\n","plt.imshow(mask, cmap=\"gray\")\n","plt.title(\"Mask Patch\")\n","plt.show()\n"],"metadata":{"id":"AVSJ8OTIr82X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["parsing dataset"],"metadata":{"id":"3WkclbL7r2zj"}},{"cell_type":"code","source":["import pandas as pd\n","from pathlib import Path\n","\n","# path ke dataset\n","DATASET_DIR = Path(\"/home/fikal/fikal/fault_detection/fault-detection-backend/dataset_fault\")\n","\n","# baca .tab files\n","train_df = pd.read_csv(DATASET_DIR / \"train.tab\", sep=\"\\t\")\n","val_df   = pd.read_csv(DATASET_DIR / \"validation.tab\", sep=\"\\t\")\n","test_df  = pd.read_csv(DATASET_DIR / \"test.tab\", sep=\"\\t\")\n","\n","print(\"🔹 Train set:\", train_df.shape)\n","print(\"🔹 Validation set:\", val_df.shape)\n","print(\"🔹 Test set:\", test_df.shape)\n","\n","# lihat contoh 5 baris pertama\n","print(\"\\nContoh train data:\")\n","print(train_df.head())\n","\n","# save ke CSV biar gampang dibaca lagi\n","train_df.to_csv(\"result_parsing/train.csv\", index=False)\n","val_df.to_csv(\"result_parsing/validation.csv\", index=False)\n","test_df.to_csv(\"result_parsing/test.csv\", index=False)\n","\n","print(\"\\n✅ Data berhasil diparse dan disimpan ke CSV\")"],"metadata":{"id":"J_I0J6-RoY5Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **training dataset**"],"metadata":{"id":"ibp38OJosMz-"}},{"cell_type":"markdown","source":["dataset class"],"metadata":{"id":"LxzeKfIJuk4c"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from PIL import Image\n","\n","class FaultDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        img_path, mask_path = row[\"image\"], row[\"mask\"]\n","\n","        # open images\n","        img = Image.open(img_path).convert(\"RGB\")\n","        mask = Image.open(mask_path).convert(\"L\")  # grayscale (binary)\n","\n","        # to tensor\n","        img = transforms.ToTensor()(img)\n","        mask = transforms.ToTensor()(mask)\n","\n","        # binarize mask (0/1)\n","        mask = (mask > 0.5).float()\n","\n","        return img, mask\n"],"metadata":{"id":"yjt2V-rvuoCP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["training"],"metadata":{"id":"_Cwc5P6fusPL"}},{"cell_type":"code","source":["import pandas as pd\n","from pathlib import Path\n","from torch.utils.data import DataLoader\n","import torch\n","import segmentation_models_pytorch as smp\n","from dataset_class import FaultDataset\n","from tqdm import tqdm\n","import torchvision.transforms as T\n","import numpy as np\n","\n","# ===============================\n","# Config\n","# ===============================\n","DATASET_DIR = Path(\"/content/dataset_fault\")  # path dataset di Colab\n","EPOCHS = 100\n","BATCH_SIZE = 8   # bisa coba naikkan ke 16 kalau GPU kuat\n","LR = 1e-4\n","PATIENCE = 7     # early stopping patience\n","CHECKPOINT_DIR = Path(\"checkpoints\")\n","CHECKPOINT_DIR.mkdir(exist_ok=True)\n","\n","# ===============================\n","# Load CSV\n","# ===============================\n","train_df = pd.read_csv(\"/content/result_parsing/train.csv\")\n","val_df   = pd.read_csv(\"/content/result_parsing/validation.csv\")\n","\n","def fix_path(path_str):\n","    filename = path_str.split(\"\\\\\")[-1]\n","    return DATASET_DIR / \"patched_images\" / filename\n","\n","train_df[\"image\"] = train_df[\"Original Image Patch\"].apply(fix_path)\n","train_df[\"mask\"]  = train_df[\"Binary Mask Image Patch\"].apply(fix_path)\n","val_df[\"image\"]   = val_df[\"Original Image Patch\"].apply(fix_path)\n","val_df[\"mask\"]    = val_df[\"Binary Mask Image Patch\"].apply(fix_path)\n","\n","# ===============================\n","# Data Augmentation\n","# ===============================\n","train_transform = T.Compose([\n","    T.Resize((512, 512)),\n","    T.RandomHorizontalFlip(),\n","    T.RandomVerticalFlip(),\n","    T.ColorJitter(brightness=0.2, contrast=0.2),\n","    T.RandomRotation(15),\n","    T.RandomAffine(degrees=15, translate=(0.05, 0.05), shear=5),\n","    T.ToTensor(),\n","])\n","\n","val_transform = T.Compose([\n","    T.Resize((512, 512)),\n","    T.ToTensor(),\n","])\n","\n","train_dataset = FaultDataset(train_df, transform=train_transform)\n","val_dataset   = FaultDataset(val_df, transform=val_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=2)\n","\n","# ===============================\n","# Device\n","# ===============================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# ===============================\n","# Model\n","# ===============================\n","model = smp.DeepLabV3Plus(\n","    encoder_name=\"resnet101\",   # backbone lebih kuat\n","    in_channels=3,\n","    classes=1,\n","    encoder_weights=\"imagenet\"\n",")\n","model.to(device)\n","\n","# Loss & Optimizer\n","bce_loss = torch.nn.BCEWithLogitsLoss()\n","dice_loss = smp.losses.DiceLoss(mode=\"binary\")\n","\n","def combined_loss(pred, target):\n","    return 0.5 * bce_loss(pred, target) + 0.5 * dice_loss(pred, target)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3, verbose=True)\n","\n","# Mixed precision\n","scaler = torch.cuda.amp.GradScaler()\n","\n","# ===============================\n","# Training Loop with Early Stopping\n","# ===============================\n","best_val_loss = np.inf\n","patience_counter = 0\n","\n","for epoch in range(1, EPOCHS+1):\n","    # ---- Train ----\n","    model.train()\n","    train_loss = 0\n","    for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [Train]\"):\n","        imgs, masks = imgs.to(device), masks.to(device)\n","\n","        optimizer.zero_grad()\n","        with torch.cuda.amp.autocast():\n","            preds = model(imgs)\n","            loss = combined_loss(preds, masks)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        train_loss += loss.item()\n","\n","    avg_train_loss = train_loss / len(train_loader)\n","\n","    # ---- Validation ----\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for imgs, masks in tqdm(val_loader, desc=f\"Epoch {epoch}/{EPOCHS} [Val]\"):\n","            imgs, masks = imgs.to(device), masks.to(device)\n","            with torch.cuda.amp.autocast():\n","                preds = model(imgs)\n","                loss = combined_loss(preds, masks)\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","    scheduler.step(avg_val_loss)\n","\n","    print(f\"📌 Epoch {epoch}/{EPOCHS} - Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n","\n","    # Save checkpoint\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        patience_counter = 0\n","        ckpt_path = CHECKPOINT_DIR / f\"deeplabv3plus_resnet101_best.pth\"\n","        torch.save(model.state_dict(), ckpt_path)\n","        print(f\"💾 New best model saved at {ckpt_path}\")\n","    else:\n","        patience_counter += 1\n","        print(f\"⏳ EarlyStopping patience: {patience_counter}/{PATIENCE}\")\n","\n","    if patience_counter >= PATIENCE:\n","        print(\"🛑 Early stopping triggered\")\n","        break\n","\n","# ===============================\n","# Save Final Model\n","# ===============================\n","Path(\"models\").mkdir(exist_ok=True)\n","torch.save(model.state_dict(), \"models/deeplabv3plus_resnet101_final.pth\")\n","print(\"✅ Training done, model saved to models/deeplabv3plus_resnet101_final.pth\")\n"],"metadata":{"id":"6uI352MXsgH-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **modeling**"],"metadata":{"id":"q_ljuLmSso_a"}},{"cell_type":"markdown","source":["inference"],"metadata":{"id":"GFIqhrSDs-gI"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from PIL import Image\n","import os\n","\n","def overlay_mask(image, mask, color=(255, 0, 0), alpha=0.5):\n","    overlay = np.array(image).copy()\n","    mask_rgb = np.zeros_like(overlay)\n","    mask_rgb[mask == 1] = color\n","    return cv2.addWeighted(overlay, 1 - alpha, mask_rgb, alpha, 0)\n","\n","def predict_mask(image_path, save_dir=\"output\"):\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # buka gambar asli\n","    img = Image.open(image_path).convert(\"RGB\")\n","    img_np = np.array(img)\n","\n","    # dummy: mask dari grayscale (nanti ganti pakai model asli)\n","    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n","    _, mask = cv2.threshold(gray, 127, 1, cv2.THRESH_BINARY)\n","\n","    # simpan mask\n","    mask_img = Image.fromarray((mask * 255).astype(np.uint8))\n","    mask_path = os.path.join(save_dir, \"mask.png\")\n","    mask_img.save(mask_path)\n","\n","    # simpan overlay\n","    overlay = overlay_mask(img, mask)\n","    overlay_img = Image.fromarray(overlay)\n","    overlay_path = os.path.join(save_dir, \"overlay.png\")\n","    overlay_img.save(overlay_path)\n","\n","    return mask_path, overlay_path\n"],"metadata":{"id":"Cp3c_OVbtAZK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["model"],"metadata":{"id":"JKs4zujvs4Jo"}},{"cell_type":"code","source":["import torch\n","import segmentation_models_pytorch as smp\n","from torchvision import transforms\n","from PIL import Image\n","from pathlib import Path\n","import numpy as np\n","import cv2\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load model (DeepLabv3+)\n","model = smp.DeepLabV3Plus(\n","    encoder_name=\"resnet50\",\n","    in_channels=3,\n","    classes=1,\n","    encoder_weights=None\n",")\n","model.load_state_dict(torch.load(\"checkpoints/deeplabv3plus_resnet50_epoch40.pth\", map_location=device))\n","model.to(device)\n","model.eval()\n","\n","# Preprocessing\n","preprocess = transforms.Compose([\n","    transforms.Resize((512, 512)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225])\n","])\n","\n","def clean_mask(mask_np, min_area=50):\n","    mask_np = (mask_np * 255).astype(np.uint8)\n","    contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    cleaned = np.zeros_like(mask_np)\n","    for cnt in contours:\n","        if cv2.contourArea(cnt) > min_area:\n","            cv2.drawContours(cleaned, [cnt], -1, 255, -1)\n","    return cleaned\n","\n","def overlay_mask(image_pil, mask_np, color=(255, 0, 0), alpha=0.5):\n","    image = np.array(image_pil.convert(\"RGB\"))\n","    mask = (mask_np > 0).astype(np.uint8)\n","    overlay = image.copy()\n","    overlay[mask == 1] = color\n","    blended = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)\n","    return Image.fromarray(blended)\n","\n","def predict_mask(image_path: str, save_dir: str = \"output\", threshold: float = 0.5):\n","    img = Image.open(image_path).convert(\"RGB\")\n","    input_tensor = preprocess(img).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        pred = model(input_tensor)\n","        pred = torch.sigmoid(pred)\n","        pred_mask = (pred > threshold).float().cpu().squeeze().numpy()\n","\n","    # resize mask ke ukuran asli\n","    mask_resized = cv2.resize(pred_mask, img.size, interpolation=cv2.INTER_NEAREST)\n","    cleaned = clean_mask(mask_resized, min_area=100)\n","    overlay_img = overlay_mask(img, cleaned)\n","\n","    # simpan hasil\n","    Path(save_dir).mkdir(parents=True, exist_ok=True)\n","    mask_path = Path(save_dir) / \"mask.png\"\n","    overlay_path = Path(save_dir) / \"overlay.png\"\n","\n","    Image.fromarray(cleaned).save(mask_path)\n","    overlay_img.save(overlay_path)\n","\n","    return str(mask_path), str(overlay_path)\n"],"metadata":{"id":"4tR5DzEdsy8S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **API test**"],"metadata":{"id":"8EQvw6UVvg8o"}},{"cell_type":"code","source":["from fastapi import FastAPI, UploadFile, File\n","from model import predict_mask\n","from tempfile import NamedTemporaryFile\n","import shutil\n","import os\n","\n","app = FastAPI()\n","\n","OUTPUT_DIR = \"output\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","@app.post(\"/predict\")\n","async def predict(file: UploadFile = File(...)):\n","    # simpan file upload ke temp\n","    with NamedTemporaryFile(delete=False, suffix=\".png\") as tmp:\n","        shutil.copyfileobj(file.file, tmp)\n","        tmp_path = tmp.name\n","\n","    # run prediction → dapat mask + overlay\n","    mask_path, overlay_path = predict_mask(tmp_path, save_dir=OUTPUT_DIR)\n","\n","    return {\n","        \"message\": \"Prediction success\",\n","        \"mask_path\": mask_path,\n","        \"overlay_path\": overlay_path\n","    }\n"],"metadata":{"id":"XGb9MqwivwGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyngrok import ngrok\n","public_url = ngrok.connect(8000)\n","print(\"🚀 Public URL:\", public_url)\n","\n","!uvicorn app:app --host 0.0.0.0 --port 8000"],"metadata":{"id":"d97gn8FXyKAK"},"execution_count":null,"outputs":[]}]}